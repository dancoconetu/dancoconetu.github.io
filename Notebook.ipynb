{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Assignment B: Social Data Analysis: NYPD Vehicle Accident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Motivation:\n",
    "\n",
    "Vehicular accidents are common. In fact they are so common that a study has actually stated that there is 1 in 606 chance of dying in vehicle accident in a lifetime. Compared to say with 174,426 chance of dying from being hit with lightning this is indeed quite a high probability. There are many different reasons and types of accidents but in the end the loss of life is always inexcusable. Our motivation for this project is therefore to analyse and show the major reasons of fatalities and injuries from vehicle accidents in a major city. We hope that our analysis and visualisation will therefore be able to show the major reasons of accidents and locations, so that in the long run people can be saved and harm be reduced. The dataset it the NewYork City Vehicle Collisions from 2012 till now. The reason why we choose this particular dataset was because of the dataset has lots of information in details regarding the vehicle collisions happened in Newyork City with the timespan, location, and the types of the vehicles and what kind of reason it cause the collisions. It can give a very good overview of from what kind of the vehicle collisions that have been occured from 2012, along with the vehicle type, in which period and how that is occured.\n",
    "\n",
    "### Basic Stats:\n",
    "\n",
    "As of 2008, the The New York State Department of Motor Vehicles  has over 10 millions vehicle registrations in force. This is practically a portion of vehicles and cars travelling within the state borders. In New York city alone of the 300000 crashes that happened in 2013, over 1000 were fatal crashes that involved vehicles, drivers, pedestrians and bicyclists.\n",
    "\n",
    "Therefore, We decided to do our project using the New York Police Department Motor Vehicle Collisions dataset. This dataset is 200MB in size and has approximately 1 million rows and 28 columns. Therefore a lot of training data is available and with the major elements of the set being zone of the accident, the type of accident, the type of vehicle, the hour and the date, we will be able to have a look at the most dangerous streets, hours and cause of accidents and if there were changes over the years, signifying that the authorities tried doing something about it.\n",
    "\n",
    "Since the basic crux of our project is the analysis of the vehicle collisions happening in New York and find out which vehicles are the safest ones, at what hour is the safest, we hope as a final result of the project to have some impact in form of visualizing the analysed data and presenting it in a format that would be understandable to the common people in order for them to understand the major risks related with traffic and that safety is always the priority since human lives cannot be replaced\n",
    "\n",
    "### Theory:\n",
    "\n",
    "One of the machine learning tools that we use is K-means clustering algorithm, which is used to solve clustering problem. The main idea of it is to define k centers, one for each cluster. These centers should be placed as much as possible  far away from each other, and thus to take each point belonging to our dataset and associate it to the nearest center. We used it for displaying April.2017 for passenger vehicles and 2016 for bicycles of different places in NewYork City. \n",
    "\n",
    "KNN is an non parametric algorithm, which means that it does not make any assumptions on the underlying data distribution. We are given some data points for training and also a new unlabelled data for testing. The algorithm has different behavior based on k.In our case, we are using k = 5,10,30.\n",
    "\n",
    "### Visualizations:\n",
    "\n",
    "We filtered the dataset with the different type of the factors for occuring the vehicle collisions, and displayed them in one heatmap for one week to check which factors have more amount and in which time period does the vehicle collisions happen more with the different type of the factors (e.g. Driver Inanttention-Distraction, Failure to Yield Right-of-Way, Fatigued or Drowsy or others).\n",
    "\n",
    "We have also filtered the dataset by different facotrs/streets and boroughs to have an overview of the percentage of the different factors/streets cause the vehicle collisions in different boroughs in NewYork. And over all, there is also K clusters for k=2-6 showing for the vehicle type of Passenger Vehicles and the bicycles and also KNN for different factors.\n",
    "\n",
    "By using them, we can have our idea well presented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"nypd.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping empty columns values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.dropna(subset=['LATITUDE'], inplace=True)\n",
    "df.dropna(subset=['LONGITUDE'], inplace=True)\n",
    "df.dropna(subset=['CONTRIBUTING FACTOR VEHICLE 1'], inplace=True)\n",
    "df = df.drop(df[df['CONTRIBUTING FACTOR VEHICLE 1'] =='Unspecified'].index)\n",
    "inattetion =  df.loc[df['CONTRIBUTING FACTOR VEHICLE 1'] == \"Driver Inattention/Distraction\"]\n",
    "failure = df.loc[df['CONTRIBUTING FACTOR VEHICLE 1'] == \"Failure to Yield Right-of-Way\"]\n",
    "otherVehicle = df.loc[df['CONTRIBUTING FACTOR VEHICLE 1'] == \"Other Vehicular\"]\n",
    "fatigued =  df.loc[df['CONTRIBUTING FACTOR VEHICLE 1'] == \"Fatigued/Drowsy\"]\n",
    "inattention25k = inattetion.iloc[-10000:]\n",
    "otherVehicle25k = otherVehicle.iloc[-10000:]\n",
    "failure25k = failure.iloc[-10000:]\n",
    "fatigued25k = fatigued.iloc[-10000:]\n",
    "minLat, minLon = 40.499186, -74.251609\n",
    "maxLat, maxLon = 40.977385, -73.648929\n",
    "minLat =  min(df[df['LATITUDE']>39]['LATITUDE'])\n",
    "maxLat = max(df[df['LATITUDE']>39]['LATITUDE'])\n",
    "minLon =  min(df[df['LONGITUDE']>-175]['LONGITUDE'])\n",
    "maxLon =  max(df[df['LONGITUDE']<-72]['LONGITUDE'])\n",
    "latIncrease = (maxLat- minLat)/50.0\n",
    "lonIncrease = (maxLon - minLon)/50.0\n",
    "import numpy as np\n",
    "Y = np.arange(minLat,maxLat, latIncrease)\n",
    "X = np.arange(minLon+0.04, maxLon+0.04, lonIncrease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Calculating the knn for Driver Inattention/Distraction, Failure to Yield Right-of-Way,Other Vehicular, Fatigued/Drowsy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### needed to calculate KNN later, took from the book\n",
    "\n",
    "from __future__ import division\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def vector_subtract(v, w):\n",
    "    \"\"\"subtracts two vectors componentwise\"\"\"\n",
    "    return [v_i - w_i for v_i, w_i in zip(v,w)]\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def magnitude(v):\n",
    "    return math.sqrt(sum_of_squares(v))\n",
    "\n",
    "def squared_distance(v, w):\n",
    "    return sum_of_squares(vector_subtract(v, w))\n",
    "\n",
    "def distance(v, w):\n",
    "   return math.sqrt(squared_distance(v, w))\n",
    "\n",
    "def majority_vote(labels):\n",
    "    \"\"\"assumes that labels are ordered from nearest to farthest\"\"\"\n",
    "    vote_counts = Counter(labels)\n",
    "    winner, winner_count = vote_counts.most_common(1)[0]\n",
    "    num_winners = len([count \n",
    "                       for count in vote_counts.values()\n",
    "                       if count == winner_count])\n",
    "\n",
    "    if num_winners == 1:\n",
    "        return winner                     # unique winner, so return it\n",
    "    else:\n",
    "        return majority_vote(labels[:-1]) # try again without the farthest\n",
    "\n",
    "\n",
    "def knn_classify(k, labeled_points, new_point):\n",
    "    \"\"\"each labeled point should be a pair (point, label)\"\"\"\n",
    "    \n",
    "    # order the labeled points from nearest to farthest\n",
    "    by_distance = sorted(labeled_points,\n",
    "                         key=lambda (point, _): distance(point, new_point))\n",
    "\n",
    "    # find the labels for the k closest\n",
    "    k_nearest_labels = [label for _, label in by_distance[:k]]\n",
    "\n",
    "    # and let them vote\n",
    "    return majority_vote(k_nearest_labels)\n",
    "allCrimes = []\n",
    "for index,row in inattention25k.iterrows():\n",
    "    allCrimes.append(([row['LATITUDE'], row['LONGITUDE']],'inattention'))\n",
    "for index,row in fatigued25k.iterrows():\n",
    "    allCrimes.append(([row['LATITUDE'], row['LONGITUDE']],'fatigued'))\n",
    "for index,row in failure25k.iterrows():\n",
    "    allCrimes.append(([row['LATITUDE'], row['LONGITUDE']],'failure'))\n",
    "for index,row in otherVehicle25k.iterrows():\n",
    "    allCrimes.append(([row['LATITUDE'], row['LONGITUDE']],'otherVehicle'))\n",
    "#creating the csv for knn=5\n",
    "inattention5 = open(\"inattention5.csv\",'w')\n",
    "inattention5.write(\"name,lat,lon\\n\")\n",
    "fatigued5 = open(\"fatigued5.csv\",'w')\n",
    "fatigued5.write(\"fatigued5,lat,lon\\n\")\n",
    "otherVehicle5 = open(\"otherVehicle5.csv\",'w')\n",
    "otherVehicle5.write(\"name,lat,lon\\n\")\n",
    "failure5 = open(\"failure5.csv\",'w')\n",
    "failure5.write(\"name,lat,lon\\n\")\n",
    "\n",
    "\n",
    "\n",
    "for y in Y:\n",
    "    for x in X: \n",
    "        c = knn_classify(5, allCrimes, [y,x])\n",
    "        if c=='inattention':\n",
    "            inattention5.write('inattention, %f, %f \\n' %( y, x))\n",
    "        if c=='fatigued':\n",
    "            fatigued5.write('fatigued5, %f, %f \\n' %( y, x))\n",
    "        if c=='otherVehicle':\n",
    "            otherVehicle5.write('otherVehicle, %f, %f \\n' %( y, x))\n",
    "        if c=='failure':\n",
    "            failure5.write('failure, %f, %f \\n' %( y, x))\n",
    "            \n",
    "        \n",
    "inattention5.close()\n",
    "fatigued5.close()\n",
    "otherVehicle5.close()\n",
    "failure5.close()\n",
    "#creating the csv for knn=10\n",
    "\n",
    "inattention10 = open(\"inattention10.csv\",'w')\n",
    "inattention10.write(\"name,lat,lon\\n\")\n",
    "fatigued10 = open(\"fatigued10.csv\",'w')\n",
    "fatigued10.write(\"fatigued10,lat,lon\\n\")\n",
    "otherVehicle10 = open(\"otherVehicle10.csv\",'w')\n",
    "otherVehicle10.write(\"name,lat,lon\\n\")\n",
    "failure10 = open(\"failure10.csv\",'w')\n",
    "failure10.write(\"name,lat,lon\\n\")\n",
    "\n",
    "for y in Y:\n",
    "    for x in X: \n",
    "        c = knn_classify(10, allCrimes, [y,x])\n",
    "        if c=='inattention':\n",
    "            inattention10.write('inattention, %f, %f \\n' %( y, x))\n",
    "        if c=='fatigued':\n",
    "            fatigued10.write('fatigued10, %f, %f \\n' %( y, x))\n",
    "        if c=='otherVehicle':\n",
    "            otherVehicle10.write('otherVehicle, %f, %f \\n' %( y, x))\n",
    "        if c=='failure':\n",
    "            failure10.write('failure, %f, %f \\n' %( y, x))\n",
    "            \n",
    "        \n",
    "inattention10.close()\n",
    "fatigued10.close()\n",
    "otherVehicle10.close()\n",
    "failure10.close()\n",
    "\n",
    "#creating the csv for knn=30\n",
    "inattention30 = open(\"inattention30.csv\",'w')\n",
    "inattention30.write(\"name,lat,lon\\n\")\n",
    "fatigued30 = open(\"fatigued30.csv\",'w')\n",
    "fatigued30.write(\"fatigued30,lat,lon\\n\")\n",
    "otherVehicle30 = open(\"otherVehicle30.csv\",'w')\n",
    "otherVehicle30.write(\"name,lat,lon\\n\")\n",
    "failure30 = open(\"failure30.csv\",'w')\n",
    "failure30.write(\"name,lat,lon\\n\")\n",
    "\n",
    "for y in Y:\n",
    "    for x in X: \n",
    "        c = knn_classify(30, allCrimes, [y,x])\n",
    "        if c=='inattention':\n",
    "            inattention30.write('inattention, %f, %f \\n' %( y, x))\n",
    "        if c=='fatigued':\n",
    "            fatigued30.write('fatigued30, %f, %f \\n' %( y, x))\n",
    "        if c=='otherVehicle':\n",
    "            otherVehicle30.write('otherVehicle, %f, %f \\n' %( y, x))\n",
    "        if c=='failure':\n",
    "            failure30.write('failure, %f, %f \\n' %( y, x))\n",
    "            \n",
    "        \n",
    "inattention30.close()\n",
    "fatigued30.close()\n",
    "otherVehicle30.close()\n",
    "failure30.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the intersections where there were the most crashes by years: 2012,2013,2014,2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfCount = df.groupby(['ON STREET NAME','CROSS STREET NAME']).size().reset_index().rename(columns={0:'count'})\n",
    "failureCount = failure.groupby(['ON STREET NAME','CROSS STREET NAME']).size().reset_index().rename(columns={0:'count'})\n",
    "fatiguedCount = fatigued.groupby(['ON STREET NAME','CROSS STREET NAME']).size().reset_index().rename(columns={0:'count'})\n",
    "inattetionCount = inattetion.groupby(['ON STREET NAME','CROSS STREET NAME']).size().reset_index().rename(columns={0:'count'})\n",
    "otherVehicleCount = otherVehicle.groupby(['ON STREET NAME','CROSS STREET NAME']).size().reset_index().rename(columns={0:'count'})\n",
    "dfCount = dfCount.sort_values('count', ascending=False)\n",
    "inattetionCount =  inattetionCount.sort_values('count', ascending=False)\n",
    "failureCount = failureCount.sort_values('count', ascending=False)\n",
    "fatiguedCount = fatiguedCount.sort_values('count', ascending=False)\n",
    "otherVehicleCount = otherVehicleCount.sort_values('count', ascending=False)\n",
    "#creating masks for each year\n",
    "mask2012 = ( pd.to_datetime(df['DATE'])  >= '2012-1-1') & ( pd.to_datetime(df['DATE'])  < '2013-1-1')\n",
    "mask2013 = ( pd.to_datetime(df['DATE'])  >= '2013-1-1') & ( pd.to_datetime(df['DATE'])  < '2014-1-1')\n",
    "mask2014 = ( pd.to_datetime(df['DATE'])  >= '2014-1-1') & ( pd.to_datetime(df['DATE'])  < '2015-1-1')\n",
    "mask2015 = ( pd.to_datetime(df['DATE'])  >= '2015-1-1') & ( pd.to_datetime(df['DATE'])  < '2016-1-1')\n",
    "mask2016 = ( pd.to_datetime(df['DATE'])  >= '2016-1-1') & ( pd.to_datetime(df['DATE'])  < '2017-1-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating small pandas for each year\n",
    "df2012 = df.loc[mask2012]\n",
    "df2013 = df.loc[mask2013]\n",
    "df2014 = df.loc[mask2014]\n",
    "df2015 = df.loc[mask2015]\n",
    "df2016 = df.loc[mask2016]\n",
    "#getting the intersections for each year and sort them descending \n",
    "dfCount2012 = df2012.groupby(['ON STREET NAME','CROSS STREET NAME']).size().reset_index().rename(columns={0:'count'})\n",
    "dfCount2012 = dfCount2012.sort_values('count', ascending=False)\n",
    "dfCount2013 = df2013.groupby(['ON STREET NAME','CROSS STREET NAME']).size().reset_index().rename(columns={0:'count'})\n",
    "dfCount2013 = dfCount2013.sort_values('count', ascending=False)\n",
    "dfCount2014 = df2014.groupby(['ON STREET NAME','CROSS STREET NAME']).size().reset_index().rename(columns={0:'count'})\n",
    "dfCount2014 = dfCount2014.sort_values('count', ascending=False)\n",
    "dfCount2015 = df2015.groupby(['ON STREET NAME','CROSS STREET NAME']).size().reset_index().rename(columns={0:'count'})\n",
    "dfCount2015 = dfCount2015.sort_values('count', ascending=False)\n",
    "dfCount2016 = df2016.groupby(['ON STREET NAME','CROSS STREET NAME']).size().reset_index().rename(columns={0:'count'})\n",
    "dfCount2016 = dfCount2016.sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#creating the csv files with the intersection data\n",
    "streetNames2012 = []\n",
    "crossStreetNames2012 = []\n",
    "count2012 = []\n",
    "counter =0 \n",
    "for i in  dfCount2012['ON STREET NAME']:\n",
    "    streetNames2012.append(i)\n",
    "    counter = counter +1 \n",
    "    if counter==10:\n",
    "        break\n",
    "counter =0 \n",
    "for i in  dfCount2012['CROSS STREET NAME']:\n",
    "    crossStreetNames2012.append(i)\n",
    "    counter = counter +1 \n",
    "    if counter==10:\n",
    "        break      \n",
    "counter =0 \n",
    "for i in  dfCount2012['count']:\n",
    "    count2012.append(i)\n",
    "    counter = counter +1 \n",
    "    if counter==10:\n",
    "        break      \n",
    "dfs = [df2013,df2014,df2015,df2016]\n",
    "count2013= []\n",
    "count2014= []\n",
    "count2015= []\n",
    "count2016= []\n",
    "countAll = [count2013,count2014,count2015,count2016]\n",
    "\n",
    "x =0 \n",
    "for dfX in dfs:\n",
    "    for i in range(0,10):\n",
    "        aMask = ( dfX['ON STREET NAME']  == streetNames2012[i]) & ( dfX['CROSS STREET NAME']  == crossStreetNames2012[i])\n",
    "        countAll[x].append( len(dfX.loc[aMask]))\n",
    "    x = x+1\n",
    "\n",
    "    \n",
    "f = open('intersections.csv', 'w')\n",
    "for i in range(0,10):\n",
    "    f.write('date,%s With %s,'%(streetNames2012[i],crossStreetNames2012[i]))\n",
    "f.write('\\n')\n",
    "years =['1-Jan-2013','1-Jan-2014','1-Jan-2015','1-Jan-2016']\n",
    "year= 0\n",
    "for i in countAll:\n",
    "    f.write('%s,'%years[year])\n",
    "    for j in range(0,10):\n",
    "        f.write('%i,'%countAll[year][j])\n",
    "    year = year + 1\n",
    "    f.write('\\n')\n",
    "f.write('1-Jan-2012,')\n",
    "for i in count2012: \n",
    "     f.write('%i,'%i)\n",
    "f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Getting the data necessary for day hour heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alltheyears = pd.date_range(start=pd.datetime(2012, 1, 1), periods=365*5, freq='D')\n",
    "w, h = 7, 24;\n",
    "Matrix = [[0 for x in range(h)] for y in range(w)] \n",
    "weekA_actualResults = []\n",
    "for i in range(0,365*5):\n",
    "    k = '{:%m/%d/%Y}'.format(alltheyears[i])\n",
    "    weekday = alltheyears[i].weekday()\n",
    "    dfPerDay  = df.loc[df['DATE']==k]\n",
    "    for j in range(0,24):\n",
    "        \n",
    "        if j<9:\n",
    "            maskH  = (dfPerDay['TIME']>= '%i:00'%j) & (dfPerDay['TIME']<='%i:00'%(j+1))\n",
    "        else:\n",
    "            if j==9:\n",
    "                maskH  = (dfPerDay['TIME']>= '%i:00'%j) & (dfPerDay['TIME']<='%i:00'%(j+1))\n",
    "            else:\n",
    "                maskH  = (dfPerDay['TIME']>= '%i:00'%j) & (dfPerDay['TIME']<='%i:00'%(j+1))\n",
    "        dfPerHour = dfPerDay.loc[maskH]\n",
    "        Matrix[weekday][j] =  Matrix[weekday][j] + len(dfPerHour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('data1.tsv', 'w')\n",
    "f.write(\"day hour value\\n\")\n",
    "for i in range(0,7):\n",
    "    for j in range(0,24):\n",
    "            s = '%i %i %i\\n'%(i,j, Matrix[i][j])\n",
    "            f.write(s)\n",
    "f.close()\n",
    "MatrixFatigued = [[0 for x in range(h)] for y in range(w)] \n",
    "for i in range(0,365*5):\n",
    "    k = '{:%m/%d/%Y}'.format(alltheyears[i])\n",
    "    weekday = alltheyears[i].weekday()\n",
    "    fatiguedPerDay  = fatigued.loc[fatigued['DATE']==k]\n",
    "    for j in range(0,24):\n",
    "        if j+1!=24:\n",
    "            fatiguedPerHour = fatiguedPerDay.between_time( '%i:00:00'%j, '%i:00:00'%((j+1)) )\n",
    "        else:\n",
    "            fatiguedPerHour = fatiguedPerDay.between_time( '%i:00:00'%j, '%i:00:00'%(0 ))\n",
    "        MatrixFatigued[weekday][j] =  MatrixFatigued[weekday][j] + len(fatiguedPerHour)\n",
    "        \n",
    "f = open('dataFatigued.tsv', 'w')\n",
    "f.write(\"day hour value\\n\")\n",
    "for i in range(0,7):\n",
    "    for j in range(0,24):\n",
    "        if(j>=2):\n",
    "            s = '%i %i %i\\n'%(i,j, MatrixFatigued[i][j])\n",
    "            f.write(s)\n",
    "f.close()\n",
    "MatrixFailure = [[0 for x in range(h)] for y in range(w)] \n",
    "for i in range(0,365*5):\n",
    "    k = '{:%m/%d/%Y}'.format(alltheyears[i])\n",
    "    weekday = alltheyears[i].weekday()\n",
    "    failurePerDay  = failure.loc[failure['DATE']==k]\n",
    "    for j in range(0,24):\n",
    "        if j+1!=24:\n",
    "            failurePerHour = failurePerDay.between_time( '%i:00:00'%j, '%i:00:00'%((j+1)) )\n",
    "        else:\n",
    "            failurePerHour = failurePerDay.between_time( '%i:00:00'%j, '%i:00:00'%((0) ))\n",
    "        MatrixFailure[weekday][j] =  MatrixFailure[weekday][j] + len(failurePerHour)\n",
    "f = open('dataFailure.tsv', 'w')\n",
    "f.write(\"day hour value\\n\")\n",
    "for i in range(0,7):\n",
    "    for j in range(0,24):\n",
    "        if(j>=2):\n",
    "            s = '%i %i %i\\n'%(i,j, MatrixFailure[i][j])\n",
    "            f.write(s)\n",
    "f.close()\n",
    "MatrixotherVehicle = [[0 for x in range(h)] for y in range(w)] \n",
    "for i in range(0,365*5):\n",
    "    k = '{:%m/%d/%Y}'.format(alltheyears[i])\n",
    "    weekday = alltheyears[i].weekday()\n",
    "    otherVehiclePerDay  = otherVehicle.loc[otherVehicle['DATE']==k]\n",
    "    for j in range(0,24):\n",
    "        if j+1!=24:\n",
    "            otherVehiclePerHour = otherVehiclePerDay.between_time( '%i:00:00'%j, '%i:00:00'%((j+1)) )\n",
    "        else: \n",
    "            otherVehiclePerHour = otherVehiclePerDay.between_time( '%i:00:00'%j, '%i:00:00'%((0)) )\n",
    "        MatrixotherVehicle[weekday][j] =  MatrixotherVehicle[weekday][j] + len(otherVehiclePerHour)\n",
    "f = open('dataotherVehicle.tsv', 'w')\n",
    "f.write(\"day hour value\\n\")\n",
    "for i in range(0,7):\n",
    "    for j in range(0,24):\n",
    "        if(j>=2):\n",
    "            s = '%i %i %i\\n'%(i,j, MatrixotherVehicle[i][j])\n",
    "            f.write(s)\n",
    "f.close()\n",
    "Matrixinattetion = [[0 for x in range(h)] for y in range(w)] \n",
    "for i in range(0,365*5):\n",
    "    k = '{:%m/%d/%Y}'.format(alltheyears[i])\n",
    "    weekday = alltheyears[i].weekday()\n",
    "    inattetionPerDay  = inattetion.loc[inattetion['DATE']==k]\n",
    "    for j in range(0,24):\n",
    "        \n",
    "        if j+1!=24:\n",
    "            inattetionPerHour = inattetionPerDay.between_time( '%i:00:00'%j, '%i:00:00'%((j+1)) )\n",
    "        else: \n",
    "            inattetionPerHour = inattetionPerDay.between_time( '%i:00:00'%j, '%i:00:00'%((0)) )\n",
    "        Matrixinattetion[weekday][j] =  Matrixinattetion[weekday][j] + len(inattetionPerHour)\n",
    "f = open('datainattetion.tsv', 'w')\n",
    "f.write(\"day hour value\\n\")\n",
    "for i in range(0,7):\n",
    "    for j in range(0,24):\n",
    "        if(j>=2):\n",
    "            s = '%i %i %i\\n'%(i,j, Matrixinattetion[i][j])\n",
    "            f.write(s)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into boroughs and calculating the percentage of the most contributing factors for each borrow, as well the street with the most crashes for each borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting the boroughs\n",
    "staten = df.loc[df['BOROUGH'] == \"STATEN ISLAND\"]\n",
    "bronx = df.loc[df['BOROUGH'] == \"BRONX\"]\n",
    "manhattan  =  df.loc[df['BOROUGH'] == \"MANHATTAN\"]\n",
    "queens =   df.loc[df['BOROUGH'] == \"QUEENS\"]\n",
    "brooklyn = df.loc[df['BOROUGH'] == \"BROOKLYN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting contributings factors for each borrow\n",
    "statenContributing = pd.value_counts(staten['CONTRIBUTING FACTOR VEHICLE 1'].values, sort=True)\n",
    "statenContributing = statenContributing[1:8]\n",
    "bronxContributing = pd.value_counts(bronx['CONTRIBUTING FACTOR VEHICLE 1'].values, sort=True)\n",
    "bronxContributing = bronxContributing[1:8]\n",
    "manhattanContributing = pd.value_counts(manhattan['CONTRIBUTING FACTOR VEHICLE 1'].values, sort=True)\n",
    "manhattanContributing = manhattanContributing[1:8]\n",
    "queensContributing = pd.value_counts(queens['CONTRIBUTING FACTOR VEHICLE 1'].values, sort=True)\n",
    "queensContributing = queensContributing[1:8]\n",
    "brooklynContributing = pd.value_counts(brooklyn['CONTRIBUTING FACTOR VEHICLE 1'].values, sort=True)\n",
    "brooklynContributing = brooklynContributing[1:8]\n",
    "#getting the streets for each borough\n",
    "statenStreets = pd.value_counts(staten['ON STREET NAME'].values, sort=True)\n",
    "statenStreets = statenStreets[0:8]\n",
    "bronxStreets = pd.value_counts(bronx['ON STREET NAME'].values, sort=True)\n",
    "bronxStreets = bronxStreets[1:9]\n",
    "manhattanStreets = pd.value_counts(manhattan['ON STREET NAME'].values, sort=True)\n",
    "manhattanStreets = manhattanStreets[0:9]\n",
    "queensStreets = pd.value_counts(queens['ON STREET NAME'].values, sort=True)\n",
    "queensStreets = queensStreets[0:9]\n",
    "brooklynStreets = pd.value_counts(brooklyn['ON STREET NAME'].values, sort=True)\n",
    "brooklynStreets = brooklynStreets[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#writting the contributing factors to the csv\n",
    "from random import randint\n",
    "f = open(\"contributing.csv\", \"w\")\n",
    "f.write(\"id,value,color\\n\")\n",
    "f.write(\"NewYork,\\n\")\n",
    "s= \"NewYork.Staten Island\"\n",
    "f.write(s)\n",
    "f.write(\",\\n\")\n",
    "for row in statenContributing.iteritems():\n",
    "    if row[0]!=\"\":\n",
    "        f.write('%s.%s,%f,%s\\n'%(s,row[0],row[1]*100.0/len(staten),'#%06X' % randint(0, 0xFFFFFF)))\n",
    "        \n",
    "s= \"NewYork.Bronx\"\n",
    "f.write(s)\n",
    "f.write(\",\\n\")\n",
    "for row in bronxContributing.iteritems():\n",
    "    if row[0]!=\"\":\n",
    "        f.write('%s.%s,%f,%s\\n'%(s,row[0],row[1]*100.0/len(bronx),'#%06X' % randint(0, 0xFFFFFF)))\n",
    "\n",
    "s= \"NewYork.Manhattan\"\n",
    "f.write(s)\n",
    "f.write(\",\\n\")\n",
    "for row in manhattanContributing.iteritems():\n",
    "    if row[0]!=\"\":\n",
    "        f.write('%s.%s,%f,%s\\n'%(s,row[0],row[1]*100.0/len(manhattan),'#%06X' % randint(0, 0xFFFFFF)))\n",
    "\n",
    "s= \"NewYork.Queens\"\n",
    "f.write(s)\n",
    "f.write(\",\\n\")\n",
    "for row in queensContributing.iteritems():\n",
    "    if row[0]!=\"\":\n",
    "        f.write('%s.%s,%f,%s\\n'%(s,row[0],row[1]*100.0/len(queens),'#%06X' % randint(0, 0xFFFFFF)))\n",
    "\n",
    "s= \"NewYork.Brooklyn\"\n",
    "f.write(s)\n",
    "f.write(\",\\n\")\n",
    "for row in brooklynContributing.iteritems():\n",
    "    if row[0]!=\"\":\n",
    "        f.write('%s.%s,%f,%s\\n'%(s,row[0],row[1]*100.0/len(brooklyn),'#%06X' % randint(0, 0xFFFFFF)))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#writting the street names for each borough on csv\n",
    "from random import randint\n",
    "f = open(\"streets.csv\", \"w\")\n",
    "f.write(\"id,value,color\\n\")\n",
    "f.write(\"NewYork,\\n\")\n",
    "s= \"NewYork.Staten Island\"\n",
    "f.write(s)\n",
    "f.write(\",\\n\")\n",
    "for row in statenStreets.iteritems():\n",
    "    if row[0]!=\"\":\n",
    "        f.write('%s.%s,%f,%s\\n'%(s,row[0],row[1]*100.0/len(staten),'#%06X' % randint(0, 0xFFFFFF)))\n",
    "        \n",
    "s= \"NewYork.Bronx\"\n",
    "f.write(s)\n",
    "f.write(\",\\n\")\n",
    "for row in bronxStreets.iteritems():\n",
    "    if row[0]!=\"\":\n",
    "        f.write('%s.%s,%f,%s\\n'%(s,row[0],row[1]*100.0/len(bronx),'#%06X' % randint(0, 0xFFFFFF)))\n",
    "\n",
    "s= \"NewYork.Manhattan\"\n",
    "f.write(s)\n",
    "f.write(\",\\n\")\n",
    "for row in manhattanStreets.iteritems():\n",
    "    if row[0]!=\"\":\n",
    "        f.write('%s.%s,%f,%s\\n'%(s,row[0],row[1]*100.0/len(manhattan),'#%06X' % randint(0, 0xFFFFFF)))\n",
    "\n",
    "s= \"NewYork.Queens\"\n",
    "f.write(s)\n",
    "f.write(\",\\n\")\n",
    "for row in queensStreets.iteritems():\n",
    "    if row[0]!=\"\":\n",
    "        f.write('%s.%s,%f,%s\\n'%(s,row[0],row[1]*100.0/len(queens),'#%06X' % randint(0, 0xFFFFFF)))\n",
    "\n",
    "s= \"NewYork.Brooklyn\"\n",
    "f.write(s)\n",
    "f.write(\",\\n\")\n",
    "for row in brooklynStreets.iteritems():\n",
    "    if row[0]!=\"\":\n",
    "        f.write('%s.%s,%f,%s\\n'%(s,row[0],row[1]*100.0/len(brooklyn),'#%06X' % randint(0, 0xFFFFFF)))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## We will make predictions for the interval 8am-8pm of the accident type of driver inattention/distraction within 2016 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get only type of driver inattention in the type of first factor within 2016\n",
    "inattetion =  df.loc[df['CONTRIBUTING FACTOR VEHICLE 1'] == \"Driver Inattention/Distraction\"]\n",
    "timespan = ( pd.to_datetime(inattetion['DATE'])  >= '2016-1-1') & ( pd.to_datetime(inattetion['DATE'])  < '2017-1-1')\n",
    "inattention2016 = inattetion.loc[timespan]\n",
    "days = pd.date_range(start=pd.datetime(2016, 1, 1), periods=366, freq='D')\n",
    "\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "# 2016 is leap year, i = 366 days, j from 8am-8pm \n",
    "#preparing the X data \n",
    "for i in range(0,366):\n",
    "    for j in range(8,20):\n",
    "        x.append(j)\n",
    "        \n",
    "#getting the Y data /target data\n",
    "for i in range(0,366):\n",
    "    k = '{:%m/%d/%Y}'.format(days[i])\n",
    "#getting hours in the right format    \n",
    "    tmp  = inattention2016.loc[inattention2016['DATE']==k]\n",
    "    for j in range(8,20):\n",
    "        if j<9:\n",
    "            hours  = (tmp['TIME']>= '0%i:00'%j) & (tmp['TIME']<='0%i:00'%(j+1))\n",
    "        else:\n",
    "            if j==9:\n",
    "                hours  = (tmp['TIME']>= '0%i:00'%j) & (tmp['TIME']<='%i:00'%(j+1))\n",
    "            else:\n",
    "                hours  = (tmp['TIME']>= '%i:00'%j) & (tmp['TIME']<='%i:00'%(j+1))\n",
    "        tmpH = tmp.loc[hours]\n",
    "        y.append(len(tmpH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go over the training data and bin and observed number of inattention per hour. There are 12 data-points per day of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##creating bins for the Y data\n",
    "data = []\n",
    "for i in range(0,12):\n",
    "    data.append([])\n",
    "\n",
    "idx = 0 \n",
    "for j in y:\n",
    "    data[idx].append(j)\n",
    "    idx = (idx+1) % 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plotting the data + the line \n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "line = plt.figure()\n",
    "xarray = np.asarray(x)\n",
    "xarray = xarray.reshape(4392,1)\n",
    "lin = linear_model.LinearRegression()\n",
    "lin.fit(xarray,y)\n",
    "alpha = lin.intercept_\n",
    "beta = lin.coef_[0]\n",
    "\n",
    "for i in range(0,12):\n",
    "    a = [i+8 for j in range(0,366)]\n",
    "    b = data[i]\n",
    "    plt.scatter(a, b, color=\"black\")\n",
    "\n",
    "plt.plot(xarray, lin.predict(xarray), color='red'\n",
    "         ,linewidth=3)\n",
    "plt.savefig('figure1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that if there is a lot going on a certain day, then more vehicle collisions in general will be happening - including more of the vehicle collisions we're looking at (e.g. INATTENTION). In order for the multiple regression to work well, it is important that the input variables have approximately the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## getting all the inattention vehicle collisions that happened in 2016\n",
    "timespan = ( pd.to_datetime(inattetion['DATE'])  >= '2016-1-1') & ( pd.to_datetime(inattetion['DATE'])  < '2017-1-1')\n",
    "inattention2016 = inattetion.loc[timespan]\n",
    "##getting the averages for all year for each hour between 8 and 20\n",
    "mean = []\n",
    "for i in range(8,20):\n",
    "    if i<9:\n",
    "        hours  = (inattention2016['TIME']>= '0%i:00'%i) & (inattention2016['TIME']<='0%i:00'%(i+1))\n",
    "    else:\n",
    "        if i==9:\n",
    "            hours  = (inattention2016['TIME']>= '0%i:00'%i) & (inattention2016['TIME']<='%i:00'%(i+1))\n",
    "        else:\n",
    "            hours  = (inattention2016['TIME']>= '%i:00'%i) & (inattention2016['TIME']<='%i:00'%(i+1))\n",
    "    everyH = inattention2016.loc[hours]\n",
    "    mean.append(len(everyH)/366)\n",
    "print \"Average inattention in each hours : \", mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### getting the average for each hour and each day. (basically the xi2) and saving into list xi2\n",
    "xi2 = []\n",
    "for i in range(0,366):\n",
    "    k = '{:%m/%d/%Y}'.format(days[i])\n",
    "    \n",
    "    tmpDays  = inattention2016.loc[inattention2016['DATE']==k]\n",
    "    for j in range(8,20):\n",
    "        if j<9:\n",
    "            hours  = (tmpDays['TIME']>= '0%i:00'%j) & (tmpDays['TIME']<='0%i:00'%(j+1))\n",
    "        else:\n",
    "            if j==9:\n",
    "                hours  = (tmpDays['TIME']>= '0%i:00'%j) & (tmpDays['TIME']<='%i:00'%(j+1))\n",
    "            else:\n",
    "                hours  = (tmpDays['TIME']>= '%i:00'%j) & (tmpDays['TIME']<='%i:00'%(j+1))\n",
    "        tmpHours = tmpDays.loc[hours]\n",
    "        xi2.append(len(tmpHours) - mean[j-8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adding data X (xi1) from linear regression to the calculated xi2 (xi2) into an array 'tmp'\n",
    "xi1 = np.array(x).reshape(4392,1)\n",
    "xi2 = np.array(xi2).reshape(4392,1)\n",
    "tmp = np.column_stack((xi1,xi2))\n",
    "#creating the model from xi1 and x2i and Y \n",
    "regrMultiple = linear_model.LinearRegression()\n",
    "regrMultiple.fit(tmp,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to plot the multiple regression with red line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "line = plt.figure()\n",
    "for i in range(0,12):\n",
    "    a = [i+8 for j in range(0,366)]\n",
    "    b = data[i]\n",
    "    plt.scatter(a, b, color=\"black\")\n",
    "plt.plot(xarray, regrMultiple.predict(tmp), color='red',\n",
    "        linewidth=3)\n",
    "plt.savefig('figure2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get only the type of at least 2 cars (passenger vehicle) vehicle collisions, drop nan for the latitude and longitude, borough, on street name, vehicle 2 factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1b2d5faa65ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcar\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'VEHICLE TYPE CODE 1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"PASSENGER VEHICLE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcar\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mcar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'VEHICLE TYPE CODE 2'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"PASSENGER VEHICLE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'LATITUDE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'LONGITUDE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BOROUGH'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "car =  df.loc[df['VEHICLE TYPE CODE 1'] == \"PASSENGER VEHICLE\"]\n",
    "car =  car.loc[car['VEHICLE TYPE CODE 2'] == \"PASSENGER VEHICLE\"]\n",
    "car.dropna(subset=['LATITUDE'], inplace=True)\n",
    "car.dropna(subset=['LONGITUDE'], inplace=True)\n",
    "car.dropna(subset=['BOROUGH'], inplace=True)\n",
    "car.dropna(subset=['ON STREET NAME'], inplace=True)\n",
    "car.dropna(subset=['CONTRIBUTING FACTOR VEHICLE 2'], inplace=True)\n",
    "\n",
    "car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get only for April 2017\n",
    "timespan = ( pd.to_datetime(car['DATE'])  >= '2017-4-1') & ( pd.to_datetime(car['DATE'])  < '2017-5-1')\n",
    "car = car.loc[timespan]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get latitude and logitude ready for creating CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for i in car['LONGITUDE']:\n",
    "    X.append(i)\n",
    "for i in car['LATITUDE']:\n",
    "    Y.append(i)\n",
    "    \n",
    "# get x and y value, if the value is out of the range, set it to the max.\n",
    "import numpy \n",
    "points = numpy.zeros(shape=(len(Y),2))\n",
    "for i in range(0,len(Y)):\n",
    "    if Y[i] <41 and X[i]<0: \n",
    "        points[i][0] = X[i]\n",
    "        points[i][1] = Y[i]\n",
    "    else:\n",
    "        points[i][0] = -74.251609\n",
    "        points[i][1]= 40.499186"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the functions from the textbook\n",
    "import numpy as np\n",
    "import random\n",
    "def cluster_points(X, mu):\n",
    "    clusters  = {}\n",
    "    for x in X:\n",
    "        bestmukey = min([(i[0], np.linalg.norm(x-mu[i[0]])) \\\n",
    "                    for i in enumerate(mu)], key=lambda t:t[1])[0]\n",
    "        try:\n",
    "            clusters[bestmukey].append(x)\n",
    "        except KeyError:\n",
    "            clusters[bestmukey] = [x]\n",
    "    return clusters\n",
    " \n",
    "def reevaluate_centers(mu, clusters):\n",
    "    newmu = []\n",
    "    keys = sorted(clusters.keys())\n",
    "    for k in keys:\n",
    "        newmu.append(np.mean(clusters[k], axis = 0))\n",
    "    return newmu\n",
    "def has_converged(mu, oldmu):\n",
    "    return (set([tuple(a) for a in mu]) == set([tuple(a) for a in oldmu]))\n",
    "\n",
    "def find_centers(X, K):\n",
    "    # Initialize to K random centers\n",
    "    oldmu = random.sample(X, K)\n",
    "    mu = random.sample(X, K)\n",
    "    while not has_converged(mu, oldmu):\n",
    "        oldmu = mu\n",
    "        # Assign all points in X to clusters\n",
    "        clusters = cluster_points(X, mu)\n",
    "        # Reevaluate centers\n",
    "        mu = reevaluate_centers(oldmu, clusters)\n",
    "    return(mu, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating CSV files for k=2,3,4,5,6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ck2 = find_centers(points,2)\n",
    "ck3 = find_centers(points,3)\n",
    "ck4 = find_centers(points,4)\n",
    "ck5 = find_centers(points,5)\n",
    "ck6 = find_centers(points,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f =  open('ck2.csv', 'w')\n",
    "f.write(\"lat,lon,type,\\n\")\n",
    "counter = 1\n",
    "\n",
    "\n",
    "# all the points besides the clusters   \n",
    "for i in range(0, len(ck2)):\n",
    "    for j in ck2[1][i]:\n",
    "        s= \"\"\n",
    "        s = s + str(j[0]) +  ','+  str(j[1]) + \",\" + str(i+1) + '\\n'\n",
    "        f.write(s)\n",
    "# the clusters points\n",
    "for i in ck2[0]:\n",
    "    s= \"\"\n",
    "    s = s + str(i[0]) +  ','+  str(i[1]) + \",\" + str(counter*10) + '\\n'\n",
    "    f.write(s )\n",
    "    counter=  counter +1 \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f =  open('ck3.csv', 'w')\n",
    "f.write(\"lat,lon,type,\\n\")\n",
    "counter = 1\n",
    "\n",
    "\n",
    "    \n",
    "for i in range(0, len(ck3)):\n",
    "    for j in ck3[1][i]:\n",
    "        s= \"\"\n",
    "        s = s + str(j[0]) +  ','+  str(j[1]) + \",\" + str(i+1) + '\\n'\n",
    "        f.write(s)\n",
    "for i in ck3[0]:\n",
    "    s= \"\"\n",
    "    s = s + str(i[0]) +  ','+  str(i[1]) + \",\" + str(counter*10) + '\\n'\n",
    "    f.write(s )\n",
    "    counter=  counter +1   \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = ck4\n",
    "\n",
    "f =  open('ck4.csv', 'w')\n",
    "f.write(\"lat,lon,type,\\n\")\n",
    "counter = 1\n",
    "for i in k[0]:\n",
    "    s= \"\"\n",
    "    s = s + str(i[0]) +  ','+  str(i[1]) + \",\" + str(counter*10) + '\\n'\n",
    "    f.write(s )\n",
    "    counter=  counter +1 \n",
    "\n",
    "    \n",
    "for i in range(0, len(k[1])):\n",
    "    for j in k[1][i]:\n",
    "        s= \"\"\n",
    "        s = s + str(j[0]) +  ','+  str(j[1]) + \",\" + str(i+1) + '\\n'\n",
    "        f.write(s)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = ck5\n",
    "\n",
    "f =  open('ck5.csv', 'w')\n",
    "f.write(\"lat,lon,type,\\n\")\n",
    "counter = 1\n",
    "for i in k[0]:\n",
    "    s= \"\"\n",
    "    s = s + str(i[0]) +  ','+  str(i[1]) + \",\" + str(counter*10) + '\\n'\n",
    "    f.write(s )\n",
    "    counter=  counter +1 \n",
    "\n",
    "    \n",
    "for i in range(0, len(k[1])):\n",
    "    for j in k[1][i]:\n",
    "        s= \"\"\n",
    "        s = s + str(j[0]) +  ','+  str(j[1]) + \",\" + str(i+1) + '\\n'\n",
    "        f.write(s)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for Vehicle Type Bicycle(Drop nan for latitude and longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = ck5\n",
    "\n",
    "f =  open('ck5.csv', 'w')\n",
    "f.write(\"lat,lon,type,\\n\")\n",
    "counter = 1\n",
    "for i in k[0]:\n",
    "    s= \"\"\n",
    "    s = s + str(i[0]) +  ','+  str(i[1]) + \",\" + str(counter*10) + '\\n'\n",
    "    f.write(s )\n",
    "    counter=  counter +1 \n",
    "\n",
    "    \n",
    "for i in range(0, len(k[1])):\n",
    "    for j in k[1][i]:\n",
    "        s= \"\"\n",
    "        s = s + str(j[0]) +  ','+  str(j[1]) + \",\" + str(i+1) + '\\n'\n",
    "        f.write(s)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the longitude and latitude ready for creating csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for i in car['LONGITUDE']:\n",
    "    X.append(i)\n",
    "for i in car['LATITUDE']:\n",
    "    Y.append(i)\n",
    "    \n",
    "# get x and y value, if the value is out of the range, set it to the max.\n",
    "import numpy \n",
    "points = numpy.zeros(shape=(len(Y),2))\n",
    "for i in range(0,len(Y)):\n",
    "    if Y[i] <41 and X[i]<0: \n",
    "        points[i][0] = X[i]\n",
    "        points[i][1] = Y[i]\n",
    "    else:\n",
    "        points[i][0] = -74.251609\n",
    "        points[i][1]= 40.499186"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the functions from the textbook\n",
    "import numpy as np\n",
    "import random\n",
    "def cluster_points(X, mu):\n",
    "    clusters  = {}\n",
    "    for x in X:\n",
    "        bestmukey = min([(i[0], np.linalg.norm(x-mu[i[0]])) \\\n",
    "                    for i in enumerate(mu)], key=lambda t:t[1])[0]\n",
    "        try:\n",
    "            clusters[bestmukey].append(x)\n",
    "        except KeyError:\n",
    "            clusters[bestmukey] = [x]\n",
    "    return clusters\n",
    " \n",
    "def reevaluate_centers(mu, clusters):\n",
    "    newmu = []\n",
    "    keys = sorted(clusters.keys())\n",
    "    for k in keys:\n",
    "        newmu.append(np.mean(clusters[k], axis = 0))\n",
    "    return newmu\n",
    "def has_converged(mu, oldmu):\n",
    "    return (set([tuple(a) for a in mu]) == set([tuple(a) for a in oldmu]))\n",
    "\n",
    "def find_centers(X, K):\n",
    "    # Initialize to K random centers\n",
    "    oldmu = random.sample(X, K)\n",
    "    mu = random.sample(X, K)\n",
    "    while not has_converged(mu, oldmu):\n",
    "        oldmu = mu\n",
    "        # Assign all points in X to clusters\n",
    "        clusters = cluster_points(X, mu)\n",
    "        # Reevaluate centers\n",
    "        mu = reevaluate_centers(oldmu, clusters)\n",
    "    return(mu, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create csv files for k=2-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ck2 = find_centers(points,2)\n",
    "ck3 = find_centers(points,3)\n",
    "ck4 = find_centers(points,4)\n",
    "ck5 = find_centers(points,5)\n",
    "ck6 = find_centers(points,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f =  open('ck2.csv', 'w')\n",
    "f.write(\"lat,lon,type,\\n\")\n",
    "counter = 1\n",
    "\n",
    "\n",
    "# all the points besides the clusters   \n",
    "for i in range(0, len(ck2)):\n",
    "    for j in ck2[1][i]:\n",
    "        s= \"\"\n",
    "        s = s + str(j[0]) +  ','+  str(j[1]) + \",\" + str(i+1) + '\\n'\n",
    "        f.write(s)\n",
    "# the clusters points\n",
    "for i in ck2[0]:\n",
    "    s= \"\"\n",
    "    s = s + str(i[0]) +  ','+  str(i[1]) + \",\" + str(counter*10) + '\\n'\n",
    "    f.write(s )\n",
    "    counter=  counter +1 \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f =  open('ck3.csv', 'w')\n",
    "f.write(\"lat,lon,type,\\n\")\n",
    "counter = 1\n",
    "\n",
    "\n",
    "    \n",
    "for i in range(0, len(ck3)):\n",
    "    for j in ck3[1][i]:\n",
    "        s= \"\"\n",
    "        s = s + str(j[0]) +  ','+  str(j[1]) + \",\" + str(i+1) + '\\n'\n",
    "        f.write(s)\n",
    "for i in ck3[0]:\n",
    "    s= \"\"\n",
    "    s = s + str(i[0]) +  ','+  str(i[1]) + \",\" + str(counter*10) + '\\n'\n",
    "    f.write(s )\n",
    "    counter=  counter +1   \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = ck4\n",
    "\n",
    "f =  open('ck4.csv', 'w')\n",
    "f.write(\"lat,lon,type,\\n\")\n",
    "counter = 1\n",
    "for i in k[0]:\n",
    "    s= \"\"\n",
    "    s = s + str(i[0]) +  ','+  str(i[1]) + \",\" + str(counter*10) + '\\n'\n",
    "    f.write(s )\n",
    "    counter=  counter +1 \n",
    "\n",
    "    \n",
    "for i in range(0, len(k[1])):\n",
    "    for j in k[1][i]:\n",
    "        s= \"\"\n",
    "        s = s + str(j[0]) +  ','+  str(j[1]) + \",\" + str(i+1) + '\\n'\n",
    "        f.write(s)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = ck5\n",
    "\n",
    "f =  open('ck5.csv', 'w')\n",
    "f.write(\"lat,lon,type,\\n\")\n",
    "counter = 1\n",
    "for i in k[0]:\n",
    "    s= \"\"\n",
    "    s = s + str(i[0]) +  ','+  str(i[1]) + \",\" + str(counter*10) + '\\n'\n",
    "    f.write(s )\n",
    "    counter=  counter +1 \n",
    "\n",
    "    \n",
    "for i in range(0, len(k[1])):\n",
    "    for j in k[1][i]:\n",
    "        s= \"\"\n",
    "        s = s + str(j[0]) +  ','+  str(j[1]) + \",\" + str(i+1) + '\\n'\n",
    "        f.write(s)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = ck6\n",
    "\n",
    "f =  open('ck6.csv', 'w')\n",
    "f.write(\"lat,lon,type,\\n\")\n",
    "counter = 1\n",
    "for i in k[0]:\n",
    "    s= \"\"\n",
    "    s = s + str(i[0]) +  ','+  str(i[1]) + \",\" + str(counter*10) + '\\n'\n",
    "    f.write(s )\n",
    "    counter=  counter +1 \n",
    "\n",
    "    \n",
    "for i in range(0, len(k[1])):\n",
    "    for j in k[1][i]:\n",
    "        s= \"\"\n",
    "        s = s + str(j[0]) +  ','+  str(j[1]) + \",\" + str(i+1) + '\\n'\n",
    "        f.write(s)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for vehicle type bicycle (drop nan for latitude and longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bike =  df.loc[df['VEHICLE TYPE CODE 1'] == \"BICYCLE\"]\n",
    "bike.dropna(subset=['LATITUDE'], inplace=True)\n",
    "bike.dropna(subset=['LONGITUDE'], inplace=True)\n",
    "bike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the longitude and latitude ready for creating csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for i in bike['LONGITUDE']:\n",
    "    X.append(i)\n",
    "for i in bike['LATITUDE']:\n",
    "    Y.append(i)\n",
    "    \n",
    "# get x and y value, if the value is out of the range, set it to the max.\n",
    "import numpy \n",
    "points = numpy.zeros(shape=(len(Y),2))\n",
    "for i in range(0,len(Y)):\n",
    "    if Y[i] <41 and X[i]<0: \n",
    "        points[i][0] = X[i]\n",
    "        points[i][1] = Y[i]\n",
    "    else:\n",
    "        points[i][0] = -74.251609\n",
    "        points[i][1]= 40.499186"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create csv files for k=2-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bk2 = find_centers(points,2)\n",
    "bk3 = find_centers(points,3)\n",
    "bk4 = find_centers(points,4)\n",
    "bk5 = find_centers(points,5)\n",
    "bk6 = find_centers(points,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f =  open('bk2.csv', 'w')\n",
    "f.write(\"lat,lon,type,\\n\")\n",
    "counter = 1\n",
    "\n",
    "\n",
    "# all the points besides the clusters   \n",
    "for i in range(0, len(bk2)):\n",
    "    for j in bk2[1][i]:\n",
    "        s= \"\"\n",
    "        s = s + str(j[0]) +  ','+  str(j[1]) + \",\" + str(i+1) + '\\n'\n",
    "        f.write(s)\n",
    "#the clusters points\n",
    "for i in bk2[0]:\n",
    "    s= \"\"\n",
    "    s = s + str(i[0]) +  ','+  str(i[1]) + \",\" + str(counter*10) + '\\n'\n",
    "    f.write(s )\n",
    "    counter=  counter +1 \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f =  open('bk3.csv', 'w')\n",
    "f.write(\"lat,lon,type,\\n\")\n",
    "counter = 1\n",
    "\n",
    "\n",
    "    \n",
    "for i in range(0, len(bk3)):\n",
    "    for j in bk3[1][i]:\n",
    "        s= \"\"\n",
    "        s = s + str(j[0]) +  ','+  str(j[1]) + \",\" + str(i+1) + '\\n'\n",
    "        f.write(s)\n",
    "for i in bk3[0]:\n",
    "    s= \"\"\n",
    "    s = s + str(i[0]) +  ','+  str(i[1]) + \",\" + str(counter*10) + '\\n'\n",
    "    f.write(s )\n",
    "    counter=  counter +1   \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = bk4\n",
    "\n",
    "f =  open('bk4.csv', 'w')\n",
    "f.write(\"lat,lon,type,\\n\")\n",
    "counter = 1\n",
    "for i in k[0]:\n",
    "    s= \"\"\n",
    "    s = s + str(i[0]) +  ','+  str(i[1]) + \",\" + str(counter*10) + '\\n'\n",
    "    f.write(s )\n",
    "    counter=  counter +1 \n",
    "\n",
    "    \n",
    "for i in range(0, len(k[1])):\n",
    "    for j in k[1][i]:\n",
    "        s= \"\"\n",
    "        s = s + str(j[0]) +  ','+  str(j[1]) + \",\" + str(i+1) + '\\n'\n",
    "        f.write(s)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = bk5\n",
    "\n",
    "f =  open('bk5.csv', 'w')\n",
    "f.write(\"lat,lon,type,\\n\")\n",
    "counter = 1\n",
    "for i in k[0]:\n",
    "    s= \"\"\n",
    "    s = s + str(i[0]) +  ','+  str(i[1]) + \",\" + str(counter*10) + '\\n'\n",
    "    f.write(s )\n",
    "    counter=  counter +1 \n",
    "\n",
    "    \n",
    "for i in range(0, len(k[1])):\n",
    "    for j in k[1][i]:\n",
    "        s= \"\"\n",
    "        s = s + str(j[0]) +  ','+  str(j[1]) + \",\" + str(i+1) + '\\n'\n",
    "        f.write(s)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = bk6\n",
    "\n",
    "f =  open('bk6.csv', 'w')\n",
    "f.write(\"lat,lon,type,\\n\")\n",
    "counter = 1\n",
    "for i in k[0]:\n",
    "    s= \"\"\n",
    "    s = s + str(i[0]) +  ','+  str(i[1]) + \",\" + str(counter*10) + '\\n'\n",
    "    f.write(s )\n",
    "    counter=  counter +1 \n",
    "\n",
    "    \n",
    "for i in range(0, len(k[1])):\n",
    "    for j in k[1][i]:\n",
    "        s= \"\"\n",
    "        s = s + str(j[0]) +  ','+  str(j[1]) + \",\" + str(i+1) + '\\n'\n",
    "        f.write(s)\n",
    "    \n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
